Metadata-Version: 2.4
Name: open-rl-trainer
Version: 0.0.1
Summary: AlphaGo-style reinforcement learning training scaffold with PyTorch.
Author: XENO-CORPORATION
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pyyaml>=6.0
Requires-Dist: numpy>=1.24
Provides-Extra: dev
Requires-Dist: pytest>=8.0; extra == "dev"
Provides-Extra: torch
Requires-Dist: torch>=2.0.0; extra == "torch"
Dynamic: license-file

# open-rl-trainer

An open-source scaffold for building AlphaGo-style reinforcement learning pipelines with PyTorch. The project supports supervised learning from expert trajectories, self-play with MCTS, and pluggable network architectures for both discrete and continuous action spaces.

## Features
- Dual training paradigms: expert pretraining **and** self-play
- Modular data adapters for JSON/Parquet trajectories
- Pluggable policy/value networks (ResNet baseline, transformer placeholder, custom heads)
- Configuration-driven runs (YAML)
- Example scripts for supervised and self-play loops
- CI workflow for linting and tests

## Getting started
```bash
python -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]"
```

Run the sample config loader test:
```bash
python -m pytest
```

## Repository layout
```
open_rl_trainer/
  data/           # datasets, preprocessing, adapters
  networks/       # base interfaces, ResNet/transformer stubs, policy heads
  training/       # supervised + self-play loops, MCTS, loss helpers
  evaluation/     # metrics and arena utilities
  utils/          # config, logging, export helpers
configs/          # example YAML configs
examples/         # quickstart scripts
tests/            # pytest suite
```

## License
MIT License. See [LICENSE](LICENSE).
